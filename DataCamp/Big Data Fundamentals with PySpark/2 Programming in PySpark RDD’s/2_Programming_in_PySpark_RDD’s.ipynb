{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2 Programming in PySpark RDD’s.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMrqmBsHWf8v8q0wpe2F0Kh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/villafue/Data-Science-at-Scale/blob/main/DataCamp/Big%20Data%20Fundamentals%20with%20PySpark/2%20Programming%20in%20PySpark%20RDD%E2%80%99s/2_Programming_in_PySpark_RDD%E2%80%99s.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6b7yvLBpyff"
      },
      "source": [
        "# Programming in PySpark RDD’s\r\n",
        "\r\n",
        "The main abstraction Spark provides is a resilient distributed dataset (RDD), which is the fundamental and backbone data type of this engine. This chapter introduces RDDs and shows how RDDs can be created and executed using RDD Transformations and Actions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEDG43Nys-Om"
      },
      "source": [
        "# Abstracting Data with RDDs\r\n",
        "\r\n",
        "1. Introduction to PySpark RDD\r\n",
        "\r\n",
        "In the first chapter, you have learned about different components of Spark namely, Spark Core, Spark SQL, and Spark MLlib. In this chapter, we will start with RDDs which are Spark’s core abstraction for working with data.\r\n",
        "2. What is RDD?\r\n",
        "\r\n",
        "Let's get started. RDD stands for Resilient Distributed Datasets. It is simply a collection of data distributed across the cluster. RDD is the fundamental and backbone data type in PySpark. When Spark starts processing data, it divides the data into partitions and distributes the data across cluster nodes, with each node containing a slice of data. Now, let's take a\r\n",
        "3. Decomposing RDDs\r\n",
        "\r\n",
        "look at the different features of RDD. The name RDD captures 3 important properties. Resilient, which means the ability to withstand failures and recompute missing or damaged partitions. Distributed, which means spanning the jobs across multiple nodes in the cluster for efficient computation. Datasets, which is a collection of partitioned data e.g. Arrays, Tables, Tuples or other objects. There are three different\r\n",
        "4. Creating RDDs. How to do it?\r\n",
        "\r\n",
        "methods for creating RDDs. You have already seen two methods in the previous chapter even though you are not aware that you are creating RDDs. The simplest method to create RDDs is to take an existing collection of objects (eg. a list, an array or a set) and pass it to SparkContext’s parallelize method. A more common way to create RDDs is to load data from external datasets such as files stored in HDFS or objects in Amazon S3 buckets or from lines in a text file stored locally and pass it to SparkContext's textFile method. Finally, RDDs can also be created from existing RDDs which we will see in the next video. In the first method,\r\n",
        "5. Parallelized collection (parallelizing)\r\n",
        "\r\n",
        "RDDs are created from a list or a set using the SparkContext’s parallelize method. Let's try and understand how RDDs are created using this method with a couple of examples. In the first example, an RDD named numRDD is created from a python list containing numbers 1, 2, 3, and 4. In the second example, an RDD named helloRDD is created from the 'hello world' string. You can confirm the object created is RDD using Python's type method. Creating\r\n",
        "6. From external datasets\r\n",
        "\r\n",
        "RDDs from external datasets is by far the most common method in PySpark. In this method, RDDs are created using SparkContext’s textFile method. In this simple example, an RDD named fileRDD is created from the lines of a README-dot-md file stored locally on your computer. Similar to previous method, you can confirm the RDD using the type method. Data\r\n",
        "7. Understanding Partitioning in PySpark\r\n",
        "\r\n",
        "partitioning is an important concept in Spark and understanding how Spark deals with partitions allow one to control parallelism. A partition in Spark is the division of the large dataset with each part being stored in multiple locations across the cluster. By default Spark partitions the data at the time of creating RDD based on several factors such as available resources, external datasets etc, however, this behavior can be controlled by passing a second argument called minPartitions which defines the minimum number of partitions to be created for an RDD. In the first example, we create an RDD named numRDD from the list of 10 integers using SparkContext's parallelize method with 6 partitions. In the second example, we create another RDD named fileRDD using SparkContext's textFile method with 6 partitions. The number of partitions in an RDD can always be found by using the getNumPartitions method. In the next\r\n",
        "8. Let's practice\r\n",
        "\r\n",
        "video, you'll see the final method of creating RDDs, for now let's create some RDDs like you just learnt. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ephzhb182yO"
      },
      "source": [
        "# RDDs from Parallelized collections\r\n",
        "\r\n",
        "Resilient Distributed Dataset (RDD) is the basic abstraction in Spark. It is an immutable distributed collection of objects. Since RDD is a fundamental and backbone data type in Spark, it is important that you understand how to create it. In this exercise, you'll create your first RDD in PySpark from a collection of words.\r\n",
        "\r\n",
        "Remember you already have a SparkContext sc available in your workspace.\r\n",
        "\r\n",
        "Instructions\r\n",
        "\r\n",
        "1. Create an RDD named RDD from a list of words.\r\n",
        "\r\n",
        "2. Confirm the object created is RDD.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0anYzJmeBT8"
      },
      "source": [
        "# Create an RDD from a list of words\r\n",
        "RDD = sc.parallelize([\"Spark\", \"is\", \"a\", \"framework\", \"for\", \"Big Data processing\"])\r\n",
        "\r\n",
        "# Print out the type of the created object\r\n",
        "print(\"The type of RDD is\", type(RDD))\r\n",
        "\r\n",
        "'''\r\n",
        "<script.py> output:\r\n",
        "    The type of RDD is <class 'pyspark.rdd.RDD'>\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zR1Unr7t9vV2"
      },
      "source": [
        "Conclusion\r\n",
        "\r\n",
        "Good job on creating your first RDD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPEMznp79ydo"
      },
      "source": [
        "# RDDs from External Datasets\r\n",
        "\r\n",
        "PySpark can easily create RDDs from files that are stored in external storage devices such as HDFS (Hadoop Distributed File System), Amazon S3 buckets, etc. However, the most common method of creating RDD's is from files stored in your local file system. This method takes a file path and reads it as a collection of lines. In this exercise, you'll create an RDD from the file path (file_path) with the file name README.md which is already available in your workspace.\r\n",
        "\r\n",
        "Remember you already have a SparkContext sc available in your workspace.\r\n",
        "\r\n",
        "Instructions\r\n",
        "\r\n",
        "1. Print the file_path in the PySpark shell.\r\n",
        "\r\n",
        "2. Create an RDD named fileRDD from a file_path with the file name README.md.\r\n",
        "\r\n",
        "3. Print the type of the fileRDD created.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBGme5UdGbxI"
      },
      "source": [
        "# Print the file_path\r\n",
        "print(\"The file_path is\", file_path)\r\n",
        "\r\n",
        "# Create a fileRDD from file_path\r\n",
        "fileRDD = sc.textFile(file_path)\r\n",
        "\r\n",
        "# Check the type of fileRDD\r\n",
        "print(\"The file type of fileRDD is\", type(fileRDD))\r\n",
        "\r\n",
        "'''\r\n",
        "<script.py> output:\r\n",
        "    The file_path is /usr/local/share/datasets/README.md\r\n",
        "    The file type of fileRDD is <class 'pyspark.rdd.RDD'>\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiS6HI7hGfmY"
      },
      "source": [
        "Conclusion\r\n",
        "\r\n",
        "Wonderful! Now you can create RDDs from Text files too!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZlt_pCjGm0q"
      },
      "source": [
        "Partitions in your data\r\n",
        "\r\n",
        "SparkContext's textFile() method takes an optional second argument called minPartitions for specifying the minimum number of partitions. In this exercise, you'll create an RDD named fileRDD_part with 5 partitions and then compare that with fileRDD that you created in the previous exercise. Refer to the \"Understanding Partition\" slide in video 2.1 to know the methods for creating and getting the number of partitions in an RDD.\r\n",
        "\r\n",
        "Remember, you already have a SparkContext sc, file_path and fileRDD available in your workspace.\r\n",
        "\r\n",
        "Instructions\r\n",
        "\r\n",
        "1. Find the number of partitions that support fileRDD RDD.\r\n",
        "\r\n",
        "2. Create an RDD named fileRDD_part from the file path but create 5 partitions.\r\n",
        "\r\n",
        "3. Confirm the number of partitions in the new fileRDD_part RDD.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-QTy_HnKuSO"
      },
      "source": [
        "# Check the number of partitions in fileRDD\r\n",
        "print(\"Number of partitions in fileRDD is\", fileRDD.getNumPartitions())\r\n",
        "\r\n",
        "# Create a fileRDD_part from file_path with 5 partitions\r\n",
        "fileRDD_part = sc.textFile(file_path, minPartitions = 5)\r\n",
        "\r\n",
        "# Check the number of partitions in fileRDD_part\r\n",
        "print(\"Number of partitions in fileRDD_part is\", fileRDD_part.getNumPartitions())\r\n",
        "\r\n",
        "'''\r\n",
        "<script.py> output:\r\n",
        "    Number of partitions in fileRDD is 2\r\n",
        "    Number of partitions in fileRDD_part is 5\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPpjr2QwKxzu"
      },
      "source": [
        "Conclusion\r\n",
        "\r\n",
        "Excellent! Note that modifying the number of partitions may result in faster performance due to parallelization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y95xs1vBLQHP"
      },
      "source": [
        "# Basic RDD Transformations and Actions\r\n",
        "\r\n",
        "1. RDD operations in PySpark\r\n",
        "\r\n",
        "In the last video, you have learned how to load your data into RDDs. In this video, you'll learn about the various operations that support RDDs in PySpark. RDDs\r\n",
        "2. Overview of PySpark operations\r\n",
        "\r\n",
        "in PySpark supports two different types of operations - Transformations and Actions. Transformations are operations on RDDs that return a new RDD and Actions are operations that perform some computation on the RDD. The most important\r\n",
        "3. RDD Transformations\r\n",
        "\r\n",
        "feature which helps RDDs in fault tolerance and optimizing resource use is the lazy evaluation. So what is lazy evaluation? Spark creates a graph from all the operations you perform on an RDD and execution of the graph starts only when an action is performed on RDD as shown in this figure. This is called lazy evaluation in Spark. The RDD transformations we will look in this video are map, filter, flatMap and union. The map\r\n",
        "4. map() Transformation\r\n",
        "\r\n",
        "transformation takes in a function and applies it to each element in the RDD. Say you have an input RDD with elements 1,2,3,4. The map transformation takes in a function and applies it to each element in the RDD with the result of the function being the new value of each element in the resulting RDD. In this example, the square function is applied to each element of the RDD. Let's understand this with an example. We first create an RDD using SparkContext's parallelize method on a list containing elements 1,2,3,4. Next, we apply map transformation for squaring each element of the RDD. The\r\n",
        "5. filter() Transformation\r\n",
        "\r\n",
        "filter transformation takes in a function and returns an RDD that only has elements that pass the condition. Suppose we have an input RDD with numbers 1,2,3,4 and we want to select numbers greater than 2, we can apply the filter transformation. Here is an example of the filter transformation wherein we use the same RDD as before to apply the filter transformation to filter out the numbers that are greater than 2. flatMap\r\n",
        "6. flatMap() Transformation\r\n",
        "\r\n",
        "is similar to map transformation except it returns multiple values for each element in the source RDD. A simple usage of flatMap is splitting up an input string into words. Here, you have an input RDD with two elements - \"hello world\" and \"how are you\". Applying the split function of the flatMap transformation results in 5 elements in the resulting RDD - \"hello\", \"world\", \"how\", \"are\", \"you\". As you can see, even though the input RDD has 2 elements, the output RDD now contains 5 elements. In this example, we create an RDD from a list containing the words \"hello world\" and \"how are you\". Next, we apply flatmap along with split function on the RDD to split the input string into individual words.\r\n",
        "7. union() Transformation\r\n",
        "\r\n",
        "union Transformation returns the union of one RDD with another RDD. In this figure, we are filtering the inputRDD and creating two RDDs - errorsRDD and warningsRDD and next we are combining both the RDDs using union transformation. To illustrate this using PySpark code, let's first create an inputRDD from a local file using SparkContext's textFile method, next we will use two filter transformations to create two RDDs errorRDD and warningsRDD and finally using union transformation we will combine them both. So far you have seen how RDD Transformations but after applying Transformations at some point, you'll want to actually do something with your dataset. This is when Actions come into picture.\r\n",
        "8. RDD Actions\r\n",
        "\r\n",
        "Actions are the operations that are applied on RDDs to return a value after running a computation. The four basic actions that you'll learn in this lesson are collect, take, first and count. Collect\r\n",
        "9. collect() and take() Actions\r\n",
        "\r\n",
        "action returns complete list of elements from the RDD. Whereas take(N) print an 'N' number of elements from the RDD. Continuing the map transformation example, executing collect returns all elements i.e 1, 4, 9, 16 from the RDD_map RDD that you created earlier. Similarly here is an example of take(2) action that prints the first 2 elements i.e 1 and 4 from the RDD_map RDD. Sometimes you just want to print the first element of\r\n",
        "10. first() and count() Actions\r\n",
        "\r\n",
        "the RDD. first action returns the first element in an RDD. It is similar to take(1). Here is an example of first action which prints the first element i.e 1 from the RDD_map RDD. Finally, the count action is used to return the total number of rows/elements in the RDD. Here is an example of count action to count the number of elements in the RDD_flatmap RDD. The result here indicates that there are 5 elements in the RDD_flatmap RDD. It's time for you to practice\r\n",
        "11. Let's practice RDD operations\r\n",
        "\r\n",
        "RDD operations in PySpark shell now. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmnkrE93P1dm"
      },
      "source": [
        "# Map and Collect\r\n",
        "\r\n",
        "The main method by which you can manipulate data in PySpark is using map(). The map() transformation takes in a function and applies it to each element in the RDD. It can be used to do any number of things, from fetching the website associated with each URL in our collection to just squaring the numbers. In this simple exercise, you'll use map() transformation to cube each number of the numbRDD RDD that you created earlier. Next, you'll return all the elements to a variable and finally print the output.\r\n",
        "\r\n",
        "Remember, you already have a SparkContext sc, and numbRDD available in your workspace.\r\n",
        "\r\n",
        "Instructions\r\n",
        "\r\n",
        "1. Create map() transformation that cubes all of the numbers in numbRDD.\r\n",
        "\r\n",
        "2. Collect the results in a numbers_all variable.\r\n",
        "\r\n",
        "3. Print the output from numbers_all variable.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrhZaS2ES4Rz"
      },
      "source": [
        "# Create map() transformation to cube numbers\r\n",
        "cubedRDD = numbRDD.map(lambda x: x ** 3)\r\n",
        "\r\n",
        "# Collect the results\r\n",
        "numbers_all = cubedRDD.collect()\r\n",
        "\r\n",
        "# Print the numbers from numbers_all\r\n",
        "for numb in numbers_all:\r\n",
        "\tprint(numb)\r\n",
        " \r\n",
        " '''\r\n",
        " <script.py> output:\r\n",
        "    1\r\n",
        "    8\r\n",
        "    27\r\n",
        "    64\r\n",
        "    125\r\n",
        "    216\r\n",
        "    343\r\n",
        "    512\r\n",
        "    729\r\n",
        "    1000\r\n",
        " '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPb724qIS97Y"
      },
      "source": [
        "Conclusion\r\n",
        "\r\n",
        "Brilliant! collect() should only be used to retrieve results for small datasets. It shouldn’t be used on large datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ewtaY3TTiji"
      },
      "source": [
        "# Filter and Count\r\n",
        "\r\n",
        "The RDD transformation filter() returns a new RDD containing only the elements that satisfy a particular function. It is useful for filtering large datasets based on a keyword. For this exercise, you'll filter out lines containing keyword Spark from fileRDD RDD which consists of lines of text from the README.md file. Next, you'll count the total number of lines containing the keyword Spark and finally print the first 4 lines of the filtered RDD.\r\n",
        "\r\n",
        "Remember, you already have a SparkContext sc, file_path and fileRDD available in your workspace.\r\n",
        "\r\n",
        "Instructions\r\n",
        "\r\n",
        "1. Create filter() transformation to select the lines containing the keyword Spark.\r\n",
        "\r\n",
        "2. How many lines in fileRDD_filter contains the keyword Spark?\r\n",
        "\r\n",
        "3. Print the first four lines of the resulting RDD.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17don8AZV4BE"
      },
      "source": [
        "# Filter the fileRDD to select lines with Spark keyword\r\n",
        "fileRDD_filter = fileRDD.filter(lambda line: 'Spark' in line.split())\r\n",
        "\r\n",
        "# How many lines are there in fileRDD?\r\n",
        "print(\"The total number of lines with the keyword Spark is\", fileRDD_filter.count())\r\n",
        "\r\n",
        "# Print the first four lines of fileRDD\r\n",
        "for line in fileRDD_filter.take(4): \r\n",
        "  print(line)\r\n",
        "\r\n",
        "'''\r\n",
        "<script.py> output:\r\n",
        "    The total number of lines with the keyword Spark is 5\r\n",
        "    Examples for Learning Spark\r\n",
        "    Examples for the Learning Spark book. These examples require a number of libraries and as such have long build files. We have also added a stand alone example with minimal dependencies and a small build file\r\n",
        "    These examples have been updated to run against Spark 1.3 so they may\r\n",
        "    * Spark 1.3\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfGyTMcfWLI0"
      },
      "source": [
        "Conclusion\r\n",
        "\r\n",
        "Well done! Note that the filter() operation does not mutate the existing fileRDD. Instead, it returns a pointer to an entirely new RDD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zK5vbIRIWQg5"
      },
      "source": [
        "# Pair RDDs in PySpark\r\n",
        "\r\n",
        "1. Working with Pair RDDs in PySpark\r\n",
        "\r\n",
        "In the last video, you were introduced to some basic RDD operations and in this video, you'll learn how to work with RDDs of key/value pairs, which are a common data type required for many operations in Spark\r\n",
        "2. Introduction to pair RDDs in PySpark\r\n",
        "\r\n",
        "Most of the real world datasets are generally key/value pairs. An example of this kind of dataset has the team name as key and the list of players as values. The typical pattern of this kind of dataset is each row is a key that maps to one or more values. In order to deal with this kind of dataset, PySpark provides a special data structure called pair RDDs. In pair RDDs, the key refers to the identifier, whereas value refers to the data.\r\n",
        "3. Creating pair RDDs\r\n",
        "\r\n",
        "There are a number of ways to create pair RDDs. The two most common ways are creating from a list of the key-value tuple or from a regular RDD. Irrespective of the method, the first step in creating pair RDDs is to get the data into key/value form. Here is an example of creating pair RDD from a list of the key-value tuple that contains the names as key and age as the value using SparkContext's parallelize method. And here is an example of creating pair RDD from regular RDDs. In this example, a regular RDD is created from a list that contains strings using SparkContext's parallelize method. Next, we create a pair RDD using map function which returns tuple with key/value pairs with key being the name and age being the value.\r\n",
        "4. Transformations on pair RDDs\r\n",
        "\r\n",
        "Pair RDDs are still RDDs and thus use all the transformations available to regular RDDs. Since pair RDDs contain tuples, we need to pass functions that operate on key-value pairs. A few special operations are available for this kind such as reduceByKey, groupByKey, sortByKey and join. Let's take a look at each of these four pair RDD transformations in detail now.\r\n",
        "5. reduceByKey() transformation\r\n",
        "\r\n",
        "The reduceByKey transformation is the most popular pair RDD transformation which combines values with the same key using a function. reduceByKey runs several parallel operations, one for each key in the dataset. Because datasets can have very large numbers of keys, reduceByKey is not implemented as an action. Instead, it returns a new RDD consisting of each key and the reduced value for that key. Here is an example of reducebykey transformation that uses a function to combine all the goals scored by each of the players. The result shows that player as key and total number of goals scored as value.\r\n",
        "6. sortByKey() transformation\r\n",
        "\r\n",
        "Sorting of data is necessary for many downstream applications. We can sort pair RDD as long as there is an ordering defined in the key. The sortByKey transformation returns an RDD sorted by key in ascending or descending order. Continuing our reduceByKey example, here is an example that sorts the data based on the number of goals scored by each player. A common use case of\r\n",
        "7. groupByKey() transformation\r\n",
        "\r\n",
        "pair RDDs is grouping the data by key. For example, viewing all of the airports for a particular country together. If the data is already keyed in the way that we want, the groupByKey operation groups all the values with the same key in the pair RDD. Here is an example of groupByKey transformation that groups all the airports for a particular country from an input list that contains list of tuples. Each tuple consists of country code and the corresponding airport code. Join transformation\r\n",
        "8. join() transformation\r\n",
        "\r\n",
        "joins two pair RDDs based on their key. Let's demonstrate this with an example. First, we create two RDDs. RDD1 contains the list of tuples with each tuple consisting of name and age and RDD2 contains the list of tuples with each tuple consisting of name and income. Applying join transformation on RDD1 and RDD2 merges two RDDs together by grouping elements with the same key. Here is an example that shows the result of join transformation of RDD1 and RDD2.\r\n",
        "9. Let's practice\r\n",
        "\r\n",
        "Now that you have learned all about pair RDDs, it's time for you to practice. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JFEjPf0ZP31"
      },
      "source": [
        "# ReduceBykey and Collect\r\n",
        "\r\n",
        "One of the most popular pair RDD transformations is reduceByKey() which operates on key, value (k,v) pairs and merges the values for each key. In this exercise, you'll first create a pair RDD from a list of tuples, then combine the values with the same key and finally print out the result.\r\n",
        "\r\n",
        "Remember, you already have a SparkContext sc available in your workspace.\r\n",
        "\r\n",
        "Instructions\r\n",
        "\r\n",
        "1. Create a pair RDD named Rdd with tuples (1,2),(3,4),(3,6),(4,5).\r\n",
        "\r\n",
        "2. Transform the Rdd with reduceByKey() into a pair RDD Rdd_Reduced by adding the values with the same key.\r\n",
        "\r\n",
        "3. Collect the contents of pair RDD Rdd_Reduced and iterate to print the output.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdvsYjnmaIVd"
      },
      "source": [
        "# Create PairRDD Rdd with key value pairs\r\n",
        "Rdd = sc.parallelize([(1,2),(3,4),(3,6),(4,5)])\r\n",
        "\r\n",
        "# Apply reduceByKey() operation on Rdd\r\n",
        "Rdd_Reduced = Rdd.reduceByKey(lambda x, y: x + y)\r\n",
        "\r\n",
        "# Iterate over the result and print the output\r\n",
        "for num in Rdd_Reduced.collect(): \r\n",
        "  print(\"Key {} has {} Counts\".format(num[0], num[1]))\r\n",
        "\r\n",
        "'''\r\n",
        "<script.py> output:\r\n",
        "    Key 1 has 2 Counts\r\n",
        "    Key 3 has 10 Counts\r\n",
        "    Key 4 has 5 Counts\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "207B78bua3xT"
      },
      "source": [
        "Conclusion\r\n",
        "\r\n",
        "Good job! reduceByKey() transformation merges the values for each key using an associative reduce function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICkRN25La5Rc"
      },
      "source": [
        "# SortByKey and Collect\r\n",
        "\r\n",
        "Many times it is useful to sort the pair RDD based on the key (for example word count which you'll see later in the chapter). In this exercise, you'll sort the pair RDD Rdd_Reduced that you created in the previous exercise into descending order and print the final output.\r\n",
        "\r\n",
        "Remember, you already have a SparkContext sc and Rdd_Reduced available in your workspace.\r\n",
        "\r\n",
        "Instructions\r\n",
        "\r\n",
        "1. Sort the Rdd_Reduced RDD using the key in descending order.\r\n",
        "\r\n",
        "2. Collect the contents and iterate to print the output.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxJeOvYfa3U8"
      },
      "source": [
        "# Sort the reduced RDD with the key by descending order\r\n",
        "Rdd_Reduced_Sort = Rdd_Reduced.sortByKey(ascending=False)\r\n",
        "\r\n",
        "# Iterate over the result and print the output\r\n",
        "for num in Rdd_Reduced_Sort.collect():\r\n",
        "  print(\"Key {} has {} Counts\".format(num[0], num[1]))\r\n",
        "\r\n",
        "'''\r\n",
        "<script.py> output:\r\n",
        "    Key 4 has 5 Counts\r\n",
        "    Key 3 has 10 Counts\r\n",
        "    Key 1 has 2 Counts\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfqpbHsydwvj"
      },
      "source": [
        "Conclusion\r\n",
        "\r\n",
        "Congratulations! You'll see how you can use sortByKey() with real world data at the end of this chapter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8R9qMDXEeGDG"
      },
      "source": [
        "# Advanced RDD Actions\r\n",
        "\r\n",
        "1. More actions\r\n",
        "\r\n",
        "Previously you learned about advanced RDD Transformations for key/value datasets. Similar to advanced RDD Transformations there are advanced RDD Actions which you'll see in this video.\r\n",
        "2. reduce() action\r\n",
        "\r\n",
        "Reduce action takes in a function which operates on two elements of the same type of RDD and returns a new element of the same type. The function should be commutative and associative so that it can be computed correctly in parallel. A simple example of such a function is +, which we can use to sum our RDD. Here is an example of reduce action that calculates the sum of all the elements in an RDD. In this example, input RDD is first created using SparkContext's parallelize method on a list consisting of numbers 1,3,4,6. Eexcuting reduce action results in 14 which is the sum of 1,3,4,6.\r\n",
        "3. saveAsTextFile() action\r\n",
        "\r\n",
        "In many cases, it is not advisable to run collect action on RDDs because of the huge size of the data. In these cases, it’s common to write data out to a distributed storage systems such as HDFS or Amazon S3. saveAsTextFile action can be used to save RDD as a text file inside a particular directory. By default, saveAsTextFile saves RDD with each partition as a separate file inside a directory. Here is an example of saveAsTextFile that saves an RDD with each partition as a separate file inside a directory. However, you can change it to return a new RDD that is reduced into a single partition using the coalesce method. Here is an example of saveAsTextFile that saves RDD as a single file inside a directory. Similar to\r\n",
        "4. Action Operations on pair RDDs\r\n",
        "\r\n",
        "pair RDD Transformations, there are also RDD Actions available for pair RDDs. However, pair RDDs also attain some additional actions of PySpark especially those that leverage the advantage of data which is of key-value nature. Let’s take a look at two pair RDD actions - countByKey and collectAsMap in this video.\r\n",
        "5. countByKey() action\r\n",
        "\r\n",
        "countByKey is only available on RDDs of type (Key, Value). With the countByKey operation, we can count the number of elements for each key. Here is an example of counting the number of values for each key in the dataset. In this example, we first create a pair RDD named rdd using SparkContext's parallelize method. Since countByKey generates a dictionary, next we iterate over the dictionary to print the each unique and number of values associated with each key as shown here. One thing to note is that countByKey should only be used on a dataset whose size is small enough to fit in memory. collectAsMap\r\n",
        "6. collectAsMap() action\r\n",
        "\r\n",
        "returns the key-value pairs in the RDD to the as a dictionary. Here is an example of collectAsMap on a pair RDD. As before we create a pair RDD using SparkContext's parallelize method and next use collectAsMap action. collectAsMap produces the key-value pairs in the RDD as a dictionary which can be used for downstream analysis. Similar to countByKey, this action should only be used if the resulting data is expected to be small, as all the data is loaded into the memory. Let's practice\r\n",
        "7. Let's practice\r\n",
        "\r\n",
        "some of these advanced Actions on some test data in PySpark shell. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPxZbBnshGm7"
      },
      "source": [
        "In [4]:\r\n",
        "Rdd.collect()\r\n",
        "Out[4]:\r\n",
        "[(1, 2), (3, 4), (3, 6), (4, 5)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIbDVRyZgwLt"
      },
      "source": [
        "# Transform the rdd with countByKey()\r\n",
        "total = Rdd.countByKey()\r\n",
        "\r\n",
        "# What is the type of total?\r\n",
        "print(\"The type of total is\", type(total))\r\n",
        "\r\n",
        "# Iterate over the total and print the output\r\n",
        "for k, v in total.items(): \r\n",
        "  print(\"key\", k, \"has\", v, \"counts\")\r\n",
        "\r\n",
        "'''\r\n",
        "<script.py> output:\r\n",
        "    The type of total is <class 'collections.defaultdict'>\r\n",
        "    key 1 has 1 counts\r\n",
        "    key 3 has 2 counts\r\n",
        "    key 4 has 1 counts\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEHkfubXlqex"
      },
      "source": [
        "Conclusion\r\n",
        "\r\n",
        "Good job! Remember unlike reduceByKey() and sortByKey(), countByKey() is an action and not a transformation on the pair RDD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckDqJ1mElu2l"
      },
      "source": [
        "# Create a base RDD and transform it\r\n",
        "\r\n",
        "The volume of unstructured data (log lines, images, binary files) in existence is growing dramatically, and PySpark is an excellent framework for analyzing this type of data through RDDs. In this 3 part exercise, you will write code that calculates the most common words from Complete Works of William Shakespeare.\r\n",
        "\r\n",
        "Here are the brief steps for writing the word counting program:\r\n",
        "\r\n",
        "    * Create a base RDD from Complete_Shakespeare.txt file.\r\n",
        "    * Use RDD transformation to create a long list of words from each element of the base RDD.\r\n",
        "    * Remove stop words from your data.\r\n",
        "    * Create pair RDD where each element is a pair tuple of ('w', 1)\r\n",
        "    * Group the elements of the pair RDD by key (word) and add up their values.\r\n",
        "    * Swap the keys (word) and values (counts) so that keys is count and value is the word.\r\n",
        "    * Finally, sort the RDD by descending order and print the 10 most frequent words and their frequencies.\r\n",
        "\r\n",
        "In this first exercise, you'll create a base RDD from Complete_Shakespeare.txt file and transform it to create a long list of words.\r\n",
        "\r\n",
        "Remember, you already have a SparkContext sc already available in your workspace. A file_path variable (which is the path to the Complete_Shakespeare.txt file) is also loaded for you.\r\n",
        "\r\n",
        "Instructions\r\n",
        "\r\n",
        "1. Create an RDD called baseRDD that reads lines from file_path.\r\n",
        "\r\n",
        "2. Transform the baseRDD into a long list of words and create a new splitRDD.\r\n",
        "\r\n",
        "3. Count the total words in splitRDD.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4xIHmC-rAZf"
      },
      "source": [
        "# Create a baseRDD from the file path\r\n",
        "baseRDD = sc.textFile(file_path)\r\n",
        "\r\n",
        "# Split the lines of baseRDD into words\r\n",
        "splitRDD = baseRDD.flatMap(lambda x: x.split())\r\n",
        "\r\n",
        "# Count the total number of words\r\n",
        "print(\"Total number of words in splitRDD:\", splitRDD.count())\r\n",
        "\r\n",
        "'''\r\n",
        "<script.py> output:\r\n",
        "    Total number of words in splitRDD: 904061\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqEu8WpYrFTO"
      },
      "source": [
        "Conclusion\r\n",
        "\r\n",
        "Good start! You have succesfully created and transformed RDD from unstructured data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8N2b6gTrJuN"
      },
      "source": [
        "# Remove stop words and reduce the dataset\r\n",
        "\r\n",
        "After splitting the lines in the file into a long list of words using flatMap() transformation, in the next step, you'll remove stop words from your data. Stop words are common words that are often uninteresting. For example \"I\", \"the\", \"a\" etc., are stop words. You can remove many obvious stop words with a list of your own. But for this exercise, you will just remove the stop words from a curated list stop_words provided to you in your environment.\r\n",
        "\r\n",
        "After removing stop words, you'll next create a pair RDD where each element is a pair tuple (k, v) where k is the key and v is the value. In this example, pair RDD is composed of (w, 1) where w is for each word in the RDD and 1 is a number. Finally, you'll combine the values with the same key from the pair RDD using reduceByKey() operation\r\n",
        "\r\n",
        "Remember you already have a SparkContext sc and splitRDD available in your workspace.\r\n",
        "\r\n",
        "Instructions\r\n",
        "\r\n",
        "1. Convert the words in splitRDD in lower case and then remove stop words from stop_words.\r\n",
        "\r\n",
        "2. Create a pair RDD tuple containing the word and the number 1 from each word element in splitRDD.\r\n",
        "\r\n",
        "3. Get the count of the number of occurrences of each word (word frequency) in the pair RDD using reduceByKey()\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDwWco0sriIR"
      },
      "source": [
        "In [1]:\r\n",
        "stop_words\r\n",
        "Out[1]:\r\n",
        "\r\n",
        "['i',\r\n",
        " 'me',\r\n",
        " 'my',\r\n",
        " 'myself',\r\n",
        " 'we',\r\n",
        " 'our',\r\n",
        " 'ours',\r\n",
        " 'ourselves',\r\n",
        " 'you',\r\n",
        " 'your',\r\n",
        " 'yours',\r\n",
        " 'yourself',\r\n",
        " 'yourselves',\r\n",
        " 'he',\r\n",
        " 'him',\r\n",
        " 'his',\r\n",
        " 'himself',\r\n",
        " 'she',\r\n",
        " 'her',\r\n",
        " 'hers',\r\n",
        " 'herself',\r\n",
        " 'it',\r\n",
        " 'its',\r\n",
        " 'itself',\r\n",
        " 'they',\r\n",
        " 'them',\r\n",
        " 'their',\r\n",
        " 'theirs',\r\n",
        " 'themselves',\r\n",
        " 'what',\r\n",
        " 'which',\r\n",
        " 'who',\r\n",
        " 'whom',\r\n",
        " 'this',\r\n",
        " 'that',\r\n",
        " 'these',\r\n",
        " 'those',\r\n",
        " 'am',\r\n",
        " 'is',\r\n",
        " 'are',\r\n",
        " 'was',\r\n",
        " 'were',\r\n",
        " 'be',\r\n",
        " 'been',\r\n",
        " 'being',\r\n",
        " 'have',\r\n",
        " 'has',\r\n",
        " 'had',\r\n",
        " 'having',\r\n",
        " 'do',\r\n",
        " 'does',\r\n",
        " 'did',\r\n",
        " 'doing',\r\n",
        " 'a',\r\n",
        " 'an',\r\n",
        " 'the',\r\n",
        " 'and',\r\n",
        " 'but',\r\n",
        " 'if',\r\n",
        " 'or',\r\n",
        " 'because',\r\n",
        " 'as',\r\n",
        " 'until',\r\n",
        " 'while',\r\n",
        " 'of',\r\n",
        " 'at',\r\n",
        " 'by',\r\n",
        " 'for',\r\n",
        " 'with',\r\n",
        " 'about',\r\n",
        " 'against',\r\n",
        " 'between',\r\n",
        " 'into',\r\n",
        " 'through',\r\n",
        " 'during',\r\n",
        " 'before',\r\n",
        " 'after',\r\n",
        " 'above',\r\n",
        " 'below',\r\n",
        " 'to',\r\n",
        " 'from',\r\n",
        " 'up',\r\n",
        " 'down',\r\n",
        " 'in',\r\n",
        " 'out',\r\n",
        " 'on',\r\n",
        " 'off',\r\n",
        " 'over',\r\n",
        " 'under',\r\n",
        " 'again',\r\n",
        " 'further',\r\n",
        " 'then',\r\n",
        " 'once',\r\n",
        " 'here',\r\n",
        " 'there',\r\n",
        " 'when',\r\n",
        " 'where',\r\n",
        " 'why',\r\n",
        " 'how',\r\n",
        " 'all',\r\n",
        " 'any',\r\n",
        " 'both',\r\n",
        " 'each',\r\n",
        " 'few',\r\n",
        " 'more',\r\n",
        " 'most',\r\n",
        " 'other',\r\n",
        " 'some',\r\n",
        " 'such',\r\n",
        " 'no',\r\n",
        " 'nor',\r\n",
        " 'not',\r\n",
        " 'only',\r\n",
        " 'own',\r\n",
        " 'same',\r\n",
        " 'so',\r\n",
        " 'than',\r\n",
        " 'too',\r\n",
        " 'very',\r\n",
        " 'can',\r\n",
        " 'will',\r\n",
        " 'just',\r\n",
        " 'don',\r\n",
        " 'should',\r\n",
        " 'now']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60hvpj4gsFeC"
      },
      "source": [
        "# Convert the words in lower case and remove stop words from stop_words\r\n",
        "splitRDD_no_stop = splitRDD.filter(lambda x: x.lower() not in stop_words)\r\n",
        "\r\n",
        "# Create a tuple of the word and 1 \r\n",
        "splitRDD_no_stop_words = splitRDD_no_stop.map(lambda w: (w, 1))\r\n",
        "\r\n",
        "# Count of the number of occurences of each word\r\n",
        "resultRDD = splitRDD_no_stop_words.reduceByKey(lambda x, y: x + y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7ftPTwzsG3u"
      },
      "source": [
        "Conclusion\r\n",
        "\r\n",
        "Good job! You are nearly ready to print the words and their frequencies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMg_F_r3sLXF"
      },
      "source": [
        "# Print word frequencies\r\n",
        "\r\n",
        "After combining the values (counts) with the same key (word), you'll print the word frequencies using the take(N) action. You could have used the collect() action but as a best practice, it is not recommended as collect() returns all the elements from your RDD. You'll use take(N) instead, to return N elements from your RDD.\r\n",
        "\r\n",
        "What if we want to return the top 10 words? For this first, you'll need to swap the key (word) and values (counts) so that keys is count and value is the word. After you swap the key and value in the tuple, you'll sort the pair RDD based on the key (count) and print the top 10 words in descending order.\r\n",
        "\r\n",
        "You already have a SparkContext sc and resultRDD available in your workspace.\r\n",
        "\r\n",
        "Instructions\r\n",
        "\r\n",
        "1. Print the first 10 words and their frequencies from the resultRDD.\r\n",
        "\r\n",
        "2. Swap the keys and values in the resultRDD.\r\n",
        "\r\n",
        "3. Sort the keys according to descending order.\r\n",
        "\r\n",
        "4. Print the top 10 most frequent words and their frequencies.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3WBsYFA1WhQ"
      },
      "source": [
        "# Display the first 10 words and their frequencies\r\n",
        "for word in resultRDD.take(10):\r\n",
        "\tprint(word)\r\n",
        "\r\n",
        "# Swap the keys and values \r\n",
        "resultRDD_swap = resultRDD.map(lambda x: (x[1], x[0]))\r\n",
        "\r\n",
        "# Sort the keys in descending order\r\n",
        "resultRDD_swap_sort = resultRDD_swap.sortByKey(ascending=False)\r\n",
        "\r\n",
        "# Show the top 10 most frequent words and their frequencies\r\n",
        "for word in resultRDD_swap_sort.take(10):\r\n",
        "\tprint(\"{} has {} counts\". format(word[1], word[0]))\r\n",
        " \r\n",
        "'''\r\n",
        "<script.py> output:\r\n",
        "    ('Quince', 1)\r\n",
        "    ('Corin,', 2)\r\n",
        "    ('circle', 10)\r\n",
        "    ('enrooted', 1)\r\n",
        "    ('divers', 20)\r\n",
        "    ('Doubtless', 2)\r\n",
        "    ('undistinguishable,', 1)\r\n",
        "    ('widowhood,', 1)\r\n",
        "    ('incorporate.', 1)\r\n",
        "    ('rare,', 10)\r\n",
        "    thou has 4247 counts\r\n",
        "    thy has 3630 counts\r\n",
        "    shall has 3018 counts\r\n",
        "    good has 2046 counts\r\n",
        "    would has 1974 counts\r\n",
        "    Enter has 1926 counts\r\n",
        "    thee has 1780 counts\r\n",
        "    I'll has 1737 counts\r\n",
        "    hath has 1614 counts\r\n",
        "    like has 1452 counts\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxgq9RBT1W4S"
      },
      "source": [
        "Conclusion\r\n",
        "\r\n",
        "Congratulations! You have sucessfully created a word count program using RDD in PySpark."
      ]
    }
  ]
}